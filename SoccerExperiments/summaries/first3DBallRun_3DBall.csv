Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
12000,1.4189383,22.204633204633204,-0.20296983,1.2208897188968768,1.2208897188968768,1.0
24000,1.4143785,23.493877551020407,-0.07610592,1.3477550521188852,1.3477550521188852,1.0
36000,1.4028288,27.657894736842106,0.07813068,1.7672248251700515,1.7672248251700515,1.0
48000,1.3899521,34.326470588235296,0.24369879,2.4332352971329407,2.4332352971329407,1.0
60000,1.3756901,49.41596638655462,0.4958769,3.9323532541258994,3.9323532541258994,1.0
72000,1.3694714,87.52941176470588,0.9013119,7.659559913417873,7.659559913417873,1.0
84000,1.3540599,177.8695652173913,1.2839042,16.08406042361605,16.08406042361605,1.0
96000,1.3438629,387.0,1.6814314,39.60000600814819,39.60000600814819,1.0
108000,1.3360853,676.1666666666666,2.1610682,65.53889894485474,65.53889894485474,1.0
120000,1.3240879,864.5,2.574186,81.79286977222988,81.79286977222988,1.0
132000,1.3097953,944.2307692307693,3.2381048,94.43847582890437,94.43847582890437,1.0
144000,1.2975461,932.6923076923077,3.7074962,93.11539840698242,93.11539840698242,1.0
156000,1.292383,923.9230769230769,3.9936671,92.40770639823033,92.40770639823033,1.0
168000,1.2870489,999.0,4.710955,100.00001525878906,100.00001525878906,1.0
